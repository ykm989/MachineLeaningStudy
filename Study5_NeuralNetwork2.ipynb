{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 흭득하는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습 문제는 데이터를 **훈련 데이터(training data)**와 **시험 데이터(test data)**로 나눠 학습과 시험을 수행하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통의 진행은 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾습니다. 그 후 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 하는 이유는 **범용 능력**을 제대로 평가하기 위함입니다.\n",
    "**범용 능력** : 학습 하지 못한 데이터도 올바르게 푸는 능력\n",
    "\n",
    "하지만 그렇다고 한 데이터셋으로만 지나치게 최적화 되면 **오버피팅**이라는 문제가 생깁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능의 '나쁨'을 나타내는 지표로 일반적으로 평군 제곱 오차와 교차 엔트로피 오차를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평균 제곱 오차  \n",
    "\n",
    "E = 1/2 * (sum(yk -tk) ** 2)  \n",
    "yk = 신경망의 출력  \n",
    "tk = 정답 레이블  \n",
    "k는 데이터의 차원 수를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, k):\n",
    "    return 0.5 * np.sum((y-k)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 엔트로피 오차  \n",
    "  \n",
    "E = -(sum(tk * log yk))  \n",
    "yk = 신경망의 출력  \n",
    "tk = 정답 레이블(one-hot-encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실상 정답 레이일 때로 추정되면 자연로그를 계산하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 값인 delta를 더하는 이유는 np.log()에 0을 입력하면 -inf가 출력된다.  \n",
    "이를 방지하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-f6e7c0610b57>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답일 시 출력되는 값이 더 낮다.  \n",
    "기계학습은 훈련 데이터에 대한 손실 함수의 값을 구해서 매개변수를 정정한다.  \n",
    "만약 100개의 훈련 데이터가 존재 한다면 각각 손실 함수를 구한 후 그 값의 합을 지표로 삼는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수의 합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교차 엔트로피 오차의 합 식  \n",
    "E = -(1/N * sum(sum(tnk * log ynk)))  \n",
    "\n",
    "N = 데이터의 갯수  \n",
    "tnk = n번째 데이터의 k번째 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터의 손실함수 값의 합을 구하는 것은 힘든다.(배치의 양이 너무 크다.)  \n",
    "그렇기 때문에 훈련 데이터의 일부만 골라 추정치를 구한다.(일부만 뽑는다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST를 이용한 미니배치 학습 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_size = x_train.shape[0] # 60000\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 트레이닝 데이터 중 batch_size만큼을 뽑는다.\n",
    "x_batch = x_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 하나당 교차 엔트로피 오차를 구하는 교차 엔트로피 오차 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = t.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shpae[0]\n",
    "    return -np.sum(np.log(y[np.arang(batch_size), t] + 1e - 7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수를 사용하는 이유  \n",
    "  \n",
    "정확도나 계단 함수를 사용하면 가중치의 작은 변화에 따른 연속적인 변화가 나타나지 않는다.  \n",
    "하지만 시그모이드 함수와 같은 손실 함수는 작은 가중치 변화에도 연속적인 변화가 나타난ㄷ.  \n",
    "시그모이드의 함수의 미분은 어느 장소라도 0이 되지는 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
